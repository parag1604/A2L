{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965734\n",
      "238872\n",
      "2204606\n",
      "1965734\n",
      "238872\n",
      "2204606\n",
      "--- 6.046639442443848 seconds ---\n",
      "2146437\n",
      "2147011\n",
      "The Republic of Croatia has a high esteem for the support of the European Parliament and for the understanding it has shown for its efforts to get closer to the European Union.\n",
      "Hay muchas historias horrorosas presentadas por la prensa para desinformar a la gente.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "base_path = \"../Dataset\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fp = open(base_path+\"/Europarl/europarl-v7.es-en.en\",'r')\n",
    "english_europarl = fp.readlines()\n",
    "print(len(english_europarl))\n",
    "fp.close()\n",
    "fp = open(base_path+\"/News_commentary/News-Commentary.en-es.en\",'r')\n",
    "english_newscommentary = fp.readlines()\n",
    "print(len(english_newscommentary))\n",
    "english_europarl.extend(english_newscommentary)\n",
    "print(len(english_europarl)) #2204606 2M\n",
    "fp.close()\n",
    "\n",
    "fp = open(base_path+\"/Europarl/europarl-v7.es-en.es\",'r')\n",
    "spanish_europarl = fp.readlines()\n",
    "print(len(spanish_europarl))\n",
    "fp.close()\n",
    "fp = open(base_path+\"/News_commentary/News-Commentary.en-es.es\",'r')\n",
    "spanish_newscommentary = fp.readlines()\n",
    "print(len(spanish_newscommentary))\n",
    "spanish_europarl.extend(spanish_newscommentary)\n",
    "print(len(spanish_europarl)) #2204606 2M\n",
    "fp.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "english_europarl = list(set(english_europarl))\n",
    "spanish_europarl = list(set(spanish_europarl))\n",
    "\n",
    "\n",
    "for i,a in enumerate(english_europarl):\n",
    "    english_europarl[i] = a.strip()\n",
    "for i,a in enumerate(spanish_europarl):\n",
    "    spanish_europarl[i] = a.strip()\n",
    "\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "\n",
    "\n",
    "print(len(english_europarl))\n",
    "print(len(spanish_europarl))\n",
    "print(english_europarl[22113])\n",
    "print(spanish_europarl[42214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "--- 918.8314793109894 seconds ---\n",
      "32455\n",
      "32869\n"
     ]
    }
   ],
   "source": [
    "# 2**13, 738 seconds, english_size-8185    ,spanish_size-8121\n",
    "# 2**14, 826 seconds, english_size-16470   ,spanish_size-16306\n",
    "# 2**15, 918 seconds, english_size-32455   ,spanish_size-32869\n",
    "\n",
    "\n",
    "\n",
    "v_size = 2**15\n",
    "\n",
    "print(\"Started\")\n",
    "start_time = time.time()\n",
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    english_europarl, target_vocab_size=v_size)\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    spanish_europarl, target_vocab_size=v_size)\n",
    "\n",
    "\n",
    "tok_en = open('tok_en_'+str(v_size)+\".pickle\", 'ab')\n",
    "pickle.dump(tokenizer_en,tok_en)\n",
    "tok_pt = open('tok_pt_'+str(v_size)+\".pickle\", 'ab')\n",
    "pickle.dump(tokenizer_pt,tok_pt)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "print(tokenizer_en.vocab_size)\n",
    "print(tokenizer_pt.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [6403, 1027, 11604, 1769, 32235, 32263, 32231, 9, 30155, 5819, 85, 13670, 32300]\n",
      "The original string: Transformer running $$@ is awesome. kone\n",
      "6403 ----> Trans\n",
      "1027 ----> former \n",
      "11604 ----> running\n",
      "1769 ---->  $\n",
      "32235 ----> $\n",
      "32263 ----> @\n",
      "32231 ---->  \n",
      "9 ----> is \n",
      "30155 ----> awe\n",
      "5819 ----> some\n",
      "85 ----> . \n",
      "13670 ----> kon\n",
      "32300 ----> e\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer running $$@ is awesome. kone'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
